{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sets and combine them into a single usable dataframe called data series\n",
    "data_series = pd.read_csv('./data/data_avg_temp.csv')\n",
    "\n",
    "# rename columns to be more descriptive\n",
    "data_series.columns = ['date', 'avg_temp', 'avg_temp_anomaly']\n",
    "data_series.drop(columns=['avg_temp_anomaly'], inplace=True)\n",
    "\n",
    "# append to the dataframe\n",
    "data_cool_degree = pd.read_csv('./data/data_cool_degree_days.csv')\n",
    "data_heat_degree = pd.read_csv('./data/data_heat_degree_days.csv')\n",
    "data_max_temp = pd.read_csv('./data/data_max_temp.csv')\n",
    "data_min_temp = pd.read_csv('./data/data_min_temp.csv')\n",
    "data_palmer_z = pd.read_csv('./data/data_palmer_z.csv')\n",
    "data_pdsi = pd.read_csv('./data/data_pdsi.csv')\n",
    "data_phdi = pd.read_csv('./data/data_phdi.csv')\n",
    "data_pmdi = pd.read_csv('./data/data_pmdi.csv')\n",
    "data_precipitation = pd.read_csv('./data/data_precipitation.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Append columns to the data series\n",
    "data_series['cool_degree_days'] = data_cool_degree['Value']\n",
    "# data_series['cool_degree_days_anomaly'] = data_cool_degree['Anomaly']\n",
    "\n",
    "data_series['heat_degree_days'] = data_heat_degree['Value']\n",
    "# data_series['heat_degree_days_anomaly'] = data_heat_degree['Anomaly']\n",
    "\n",
    "data_series['max_temp'] = data_max_temp['Value']\n",
    "# data_series['max_temp_anomaly'] = data_max_temp['Anomaly']\n",
    "\n",
    "data_series['min_temp'] = data_min_temp['Value']\n",
    "# data_series['min_temp_anomaly'] = data_min_temp['Anomaly']\n",
    "\n",
    "data_series['palmer_z'] = data_palmer_z['Value']\n",
    "# data_series['palmer_z_anomaly'] = data_palmer_z['Anomaly']\n",
    "\n",
    "data_series['pdsi'] = data_pdsi['Value']\n",
    "# data_series['pdsi_anomaly'] = data_pdsi['Anomaly']\n",
    "\n",
    "data_series['phdi'] = data_phdi['Value']\n",
    "# data_series['phdi_anomaly'] = data_phdi['Anomaly']\n",
    "\n",
    "data_series['pmdi'] = data_pmdi['Value']\n",
    "# data_series['pmdi_anomaly'] = data_pmdi['Anomaly']\n",
    "\n",
    "data_series['precipitation'] = data_precipitation['Value']\n",
    "# data_series['precipitation_anomaly'] = data_precipitation['Anomaly']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy array\n",
    "avg_temp = data_series['avg_temp'].to_numpy()\n",
    "data_series.drop(columns=['avg_temp'], inplace=True)\n",
    "\n",
    "x = data_series.to_numpy()\n",
    "\n",
    "# Expand first dim\n",
    "x = np.expand_dims(x, axis=0)\n",
    "avg_temp = np.expand_dims(avg_temp, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marienasmiscellanous/miniconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x297250f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the GRU model \n",
    "model_gru = Sequential( ##Sequential model \n",
    "    [\n",
    "        GRU(20, activation='relu', input_shape=(x.shape[1], x.shape[2])), ##20 blocks of GRU layer\n",
    "        Dense(1) ##1 block of Dense layer, so it produces one node which is one output\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='mse') \n",
    "model_gru.fit(x, avg_temp, epochs=300, batch_size=1, verbose=0) ##x is the dataset, feed it 300 times, we see what the avg temp is, and then we use the MSE (loss function) and use stochastic gradient \n",
    "#descent of adam optimizer, which is standard for many ML models. We repeat it, which is the training procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAM cool_degree_days\n",
      "PARAM heat_degree_days\n",
      "PARAM max_temp\n",
      "PARAM min_temp\n",
      "PARAM palmer_z\n",
      "PARAM pdsi\n",
      "PARAM phdi\n",
      "PARAM pmdi\n",
      "PARAM precipitation\n"
     ]
    }
   ],
   "source": [
    "# Predict the temperature by regressing the other features and then predicting average temperature\n",
    "parameters = ['cool_degree_days', 'heat_degree_days', 'max_temp', \n",
    "              'min_temp', 'palmer_z', 'pdsi',\n",
    "              'phdi', 'pmdi', 'precipitation']\n",
    "# parameters = ['cool_degree_days']\n",
    "models = {}\n",
    "scalers = {}\n",
    "\n",
    "date = data_series['date'].to_numpy()\n",
    "date_splice = np.zeros((len(date), 2))\n",
    "\n",
    "param_models = []\n",
    "\n",
    "for i, element in enumerate(date):\n",
    "    # print(element)\n",
    "    # Splice this string to get year and month\n",
    "    date_splice[i,0] = int(str(element)[0:4]) ##year \n",
    "    date_splice[i,1] = int(str(element)[5:7]) ##month\n",
    "\n",
    "for param in parameters: ##for every parameter, we train the model. We do this 9 times because we have 9 parameters \n",
    "    print(f\"PARAM\", param)\n",
    "    y = data_series[param].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(date_splice, y, test_size=0.2, shuffle=False, verbose=0) ##splits data to training and testing\n",
    "\n",
    "    model = Sequential([ ##Dense nueral network, consisting of the two inputs year and month, \n",
    "        Dense(5, activation='relu', input_shape=(X_train.shape[1],)),  # input_shape=(2,)\n",
    "        Dense(1)  # Output layer\n",
    "    ]) ##defines structure of model  \n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse') ##\n",
    "    model.fit(X_train, y_train, epochs=300, batch_size=16, validation_data=(X_test, y_test),verbose=0) ##training loop \n",
    "\n",
    "    # Save model\n",
    "    param_models.append(model) ##collect all the models into param_model\n",
    "\n",
    "\n",
    "#Write a script that runs 1000 times (Monte Carlo), and average out the simulations over time, and plot them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     date_predict[i, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2024\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     11\u001b[0m     date_predict[i, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[0;32m---> 14\u001b[0m param_predicts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  Predict the parameters.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(param_models):\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Prediction of the average temperature\n",
    "# Using the trained models to predict the parameters and then use those predictions as input for the GRU model\n",
    "\n",
    "# Predict the parameters for the next 5 year * 12 months (60 datapoints)\n",
    "\n",
    "# Get dates for prediction (202401-202812)\n",
    "years = 60\n",
    "date_predict = np.zeros((years*12, 2)) ##for year and month prediction \n",
    "for i in range(years*12):\n",
    "    date_predict[i, 0] = 2024 + i // 12 ##year \n",
    "    date_predict[i, 1] = 1 + i % 12 ##month\n",
    "\n",
    "\n",
    "param_predicts = np.zeros((len(parameters),years*12))\n",
    "\n",
    "#  Predict the parameters.\n",
    "for k, model in enumerate(param_models):\n",
    "    for i in range(years*12):\n",
    "        answer = model.predict(date_predict[i].reshape(1, -1)) ##produces predicted parameter output\n",
    "        param_predicts[k, i] = answer\n",
    "\n",
    "yearmonth_vec = np.zeros((years*12,1))\n",
    "for i in range(years*12):\n",
    "    yearmonth_vec[i] = date_predict[i, 0] * 100 + date_predict[i, 1] ##combined year and month value\n",
    "\n",
    "params_predicts = np.append(yearmonth_vec, param_predicts.T, axis=1, verbose=0)\n",
    "\n",
    "print(params_predicts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(years*12):\n",
    "    print(f\"Year: {int(params_predicts[i,0]//100)} Month: {int(params_predicts[i,0]%100)}\")\n",
    "    print(f\"Predicted parameters:\")\n",
    "    for j, param in enumerate(parameters):\n",
    "        print(f\"{param}: {params_predicts[i,j+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use predicted params for GRU input to predict avg_temp\n",
    "predictions = []\n",
    "\n",
    "for i in range(years*12):\n",
    "    new_timestep = params_predicts[i,:]\n",
    "    new_timestep = np.expand_dims(new_timestep, axis=0)\n",
    "    x = np.append(x[:,1:,:], np.expand_dims(new_timestep, axis=0), axis=1)\n",
    "\n",
    "    # x = np.expand_dims(params_predicts[i, :], axis=0)\n",
    "    avg_temp_predict = model_gru.predict(x) ##Here's the money \n",
    "    predictions.append(avg_temp_predict)\n",
    "\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "predictions = np.array(predictions).reshape(-1)\n",
    "plt.plot(yearmonth_vec[2:], predictions[2:], label='Predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
